{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFAutoModelForSequenceClassification, AutoConfig, TFAutoModel\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Geotrend/bert-base-ru-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at Geotrend/bert-base-ru-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained('Geotrend/bert-base-ru-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('Geotrend/bert-base-ru-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/qa_route_specs.txt', 'r') as source:\n",
    "    text = source.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'Маршрут \"(.+?)\"')\n",
    "names = [re.search(pattern, route).group(1) for route in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tokenized = tokenizer(names, max_length=20, truncation=True, \n",
    "                   padding='max_length', return_token_type_ids=False, return_tensors='tf')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(titles_tokenized)\n",
    "batched_dataset = dataset.batch(batch_size=128)\n",
    "result = [model(batch, training=False)[1] for batch in batched_dataset]\n",
    "titles_vectorized = tf.concat(result, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инференс ускорили. Теперь попробуем разогнать расчет дистанций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "def match(name):\n",
    "    tokens2 = tokenizer([name], max_length=20, truncation=True, \n",
    "                   padding='max_length', return_token_type_ids=False, return_tensors='tf')\n",
    "    out2 = model(**tokens2)\n",
    "    populated_result = tf.repeat(out2[1], titles_vectorized.shape[0], axis=0)\n",
    "    distances = tf.sqrt(tf.reduce_sum(tf.square(populated_result - titles_vectorized), 1))\n",
    "    length = np.min(distances)\n",
    "    index = np.argmin(distances)\n",
    "    return index, length\n",
    "\n",
    "def variants(sentence):\n",
    "    result = []\n",
    "    splitted = sentence.split()\n",
    "    for i in range(len(splitted)):\n",
    "        left_slice = splitted[i:]\n",
    "        for j in range(i+1, len(left_slice) + i + 1):\n",
    "            result.append(splitted[i:j])\n",
    "    return result\n",
    "    \n",
    "def match_sentence(sentence, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    perms = variants(sentence)\n",
    "    best_match, best_score = -1, 999999\n",
    "    for var in perms:\n",
    "        processed = [lemmer.parse(token)[0].normal_form for token in var] if lemmer else var\n",
    "        text = ' '.join(processed)\n",
    "        index, length = match(text.lower())\n",
    "        if length < best_score:\n",
    "            best_match = index\n",
    "            best_score = length\n",
    "    \n",
    "    print(f'{names[best_match]}, {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чемпион колхоза, 0.0016712704673409462\n",
      "CPU times: user 1.04 s, sys: 20.3 ms, total: 1.06 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая длина трассы чемпион колхоза', lemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чемпион колхоза, 0.0016712704673409462\n",
      "CPU times: user 235 ms, sys: 67 µs, total: 235 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('чемпион колхоза', lemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "любовь на кончиках пончика, 1.8347655534744263\n",
      "CPU times: user 1.07 s, sys: 20.1 ms, total: 1.09 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая категория у химии любви', lemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поняятно... формы слов в русском языке подводят.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "химия любви, 0.0016301103169098496\n",
      "CPU times: user 1.05 s, sys: 16.3 ms, total: 1.07 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая категория у химия любви', lemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Окей попробуем лемматизировать исходник"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    return [' '.join([lemmer.parse(token)[0].normal_form.lower() for token in sent.split()]) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tokenized = tokenizer(preprocess(names), max_length=20, truncation=True, \n",
    "                   padding='max_length', return_token_type_ids=False, return_tensors='tf')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(titles_tokenized)\n",
    "batched_dataset = dataset.batch(batch_size=128)\n",
    "result = [model(batch, training=False)[1] for batch in batched_dataset]\n",
    "titles_vectorized = tf.concat(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химия любви, 0.00162822799757123\n",
      "CPU times: user 1.09 s, sys: 20.3 ms, total: 1.11 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая категория у химии любви')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химера, 0.001562767312861979\n",
      "CPU times: user 710 ms, sys: 4.09 ms, total: 714 ms\n",
      "Wall time: 708 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая длина у Химеры')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Крымский геккон, 0.0016894518630579114\n",
      "CPU times: user 434 ms, sys: 12.2 ms, total: 446 ms\n",
      "Wall time: 441 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('категория крымского геккона')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вентовка, 0.001521328929811716\n",
      "CPU times: user 1.47 s, sys: 8.22 ms, total: 1.48 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('сколько нужно веревки на маршрут вентовка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Японский хрен, 0.0014782077632844448\n",
      "CPU times: user 1.02 s, sys: 24.1 ms, total: 1.04 s\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('какая станция у японского хрена')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мисячне сяйво, 0.9865515828132629\n"
     ]
    }
   ],
   "source": [
    "match_sentence('сколько питчей у мисячне сяйва')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
