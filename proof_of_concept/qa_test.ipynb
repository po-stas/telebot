{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039c8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:06:15.640 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_ru_rubert' as '/home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/configs/squad/squad_ru_rubert.json'\n",
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-iystdaqw\n",
      "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-iystdaqw\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23580 sha256=b16ec23453bbcec8490f1f30e943857410902247c191f6850a8b9b7e0ce1c69f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jhbucuwd/wheels/65/8c/cb/bb585d79f235c48068b6f4f70f99118fedc3d964d16b1e2caa\n",
      "Successfully built bert-dp\n",
      "Installing collected packages: bert-dp\n",
      "Successfully installed bert-dp-1.0\n",
      "Collecting tensorflow==1.15.2\n",
      "  Downloading tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 110.5 MB 6.2 MB/s eta 0:00:01    |██████████████▏                 | 48.7 MB 213 kB/s eta 0:04:50\n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 4.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 633 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 742 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.17.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from tensorflow==1.15.2) (1.18.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from tensorflow==1.15.2) (0.36.2)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from tensorflow==1.15.2) (1.15.0)\n",
      "Requirement already satisfied: h5py in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (52.0.0.post20210125)\n",
      "Requirement already satisfied: importlib-metadata in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.0.1)\n",
      "Requirement already satisfied: dataclasses in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
      "Building wheels for collected packages: gast, termcolor, wrapt\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=9deb6c308e9e206e8f3c08e1728fe6fc8e2239d37efa18e947c19f54bedc4f5f\n",
      "  Stored in directory: /home/postas/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=63b7732a6f04ac564942772c030afa20a3480fe17187b9977e8841a56cffc2e2\n",
      "  Stored in directory: /home/postas/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=76046 sha256=4cea3351cc17e528187fb9d1f7620ebbccfbb1de11bb7d1e56e21e181222c9a2\n",
      "  Stored in directory: /home/postas/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built gast termcolor wrapt\n",
      "Installing collected packages: werkzeug, protobuf, markdown, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 grpcio-1.38.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 protobuf-3.17.1 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 werkzeug-2.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!python -m deeppavlov install squad_ru_rubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807823b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 341 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (0.0.35)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 173 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (1.18.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: dataclasses in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: filelock in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: six in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 regex-2021.4.4 tokenizers-0.10.3 transformers-4.6.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50916c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "from deeppavlov.core.common.file import read_json\n",
    "import torch\n",
    "import pymorphy2\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848b60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFAutoModel, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd173819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/postas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/postas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/postas/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/postas/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:81: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:178: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:154: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:166: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:89: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/models/bert/bert_squad.py:94: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-30 23:52:56.435 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/postas/.deeppavlov/models/squad_ru_bert/model_rubert]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/postas/anaconda3/envs/deeppavlov/lib/python3.6/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/postas/.deeppavlov/models/squad_ru_bert/model_rubert\n"
     ]
    }
   ],
   "source": [
    "squad_model = build_model(configs.squad.squad_ru_rubert, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ba0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19d00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e99761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c85b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer('химера', padding='max_length', truncation=True, max_length=20, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f5a6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 43667,  4475,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "763b751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a0b82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2461,  0.3397, -0.8128,  ..., -0.2800,  0.8480, -0.1490]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc5ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = torch.nn.PairwiseDistance(p=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dede75de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdist(model_output[1], model_output[1]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abd3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/qa_route_specs.txt', 'r') as source:\n",
    "    text = source.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf12ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'Маршрут \"(.+?)\"')\n",
    "names = [re.search(pattern, route).group(1) for route in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7bb305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    return [' '.join([lemmer.parse(token)[0].normal_form.lower() for token in sent.split()]) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae5df990",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tokenized = tokenizer(preprocess(names), padding='max_length',\n",
    "                             truncation=True, max_length=20, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a515249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitlesDataset(Dataset):\n",
    "    def __init__(self, titles):\n",
    "        self.titles = titles\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.titles['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.titles['input_ids'][idx], \n",
    "                'token_type_ids': self.titles['token_type_ids'][idx],\n",
    "                'attention_mask': self.titles['attention_mask'][idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81442dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32818"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TitlesDataset(titles_tokenized)\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f2ed51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([101, 159, 774, 378, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446365db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 43667,  4475,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73c96bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "509f3169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   159,   774,  ...,     0,     0,     0],\n",
       "         [  101, 20981,  1918,  ...,     0,     0,     0],\n",
       "         [  101,  1886,   978,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 55987,   102,  ...,     0,     0,     0],\n",
       "         [  101, 22178,   102,  ...,     0,     0,     0],\n",
       "         [  101, 64483,   102,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(data_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "725489e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([101, 159, 774, 378, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db8718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = iter(data_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8203d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model(input_ids=tens['input_ids'], attention_mask=tens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76be52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b07b7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [37:47<00:00,  8.82s/it]\n"
     ]
    }
   ],
   "source": [
    "titles_vectorized = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader):\n",
    "        titles_vectorized.append(model(**batch)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c09d69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как же это долго.. все дело видимо в том, что в этом энвайронменте нет подобающей версии cuda... \n",
    "# надо делать энв со старой кудой, что не хочется, поскольку и так куча ограничений изза диппавлова.\n",
    "# На tf c GPU эта операция занимает меньше чем полминуты.\n",
    "# Но по факту, работать эта штука будет на серваке, где никакой куды нет. Есть ли там вообще видеокарта и то вопрос\n",
    "# Поэтому если делать это скажем раз в сутки, то не жалко, снизить количество ядер и пусть часок в сутки тратит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29670b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0e2f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_vectorized[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa4a8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_vectorized_tensor = torch.cat(titles_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf72caaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32818, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_vectorized_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c9c8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним это"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd83cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(titles_vectorized_tensor, 'datasets/titles_vectorized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dbb2d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем вычислить дистанции. Сформируем эмбеддинг запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8183760",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer('химера', padding='max_length', truncation=True, max_length=20, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e13a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ea0db53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2461,  0.3397, -0.8128,  ..., -0.2800,  0.8480, -0.1490]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc1c81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем 32К одинаковых тензоров для векторного вычисления парной дистанции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3a14910",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = torch.cat(titles_vectorized_tensor.shape[0]*[out[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c2762e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32818, 1024])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4b13299",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = torch.nn.PairwiseDistance(p=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdcd7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pdist(outs, titles_vectorized_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32a71514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32818,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2490793f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb91ee86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# точность\n",
    "dists[384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb61c959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Химера'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4fe4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Победа! )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfda7f7",
   "metadata": {},
   "source": [
    "## Обернем все в функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fcedba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(sentence, pdist=torch.nn.PairwiseDistance(p=8.0)):\n",
    "    encoded_input = tokenizer(sentence, max_length=20, truncation=True, \n",
    "                   padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded_input)\n",
    "    \n",
    "    populated_result = torch.cat(titles_vectorized_tensor.shape[0]*[out[1]])\n",
    "    distances = pdist(populated_result, titles_vectorized_tensor).numpy()\n",
    "    length = np.min(distances)\n",
    "    index = np.argmin(distances)\n",
    "    return index, length\n",
    "\n",
    "def variants(sentence):\n",
    "    result = []\n",
    "    splitted = sentence.split()\n",
    "    for i in range(len(splitted)):\n",
    "        left_slice = splitted[i:]\n",
    "        for j in range(i+1, len(left_slice) + i + 1):\n",
    "            result.append(splitted[i:j])\n",
    "    return result\n",
    "    \n",
    "def match_sentence(sentence, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    perms = variants(sentence)\n",
    "    best_match, best_score = -1, 999999\n",
    "    for var in perms:\n",
    "        processed = [lemmer.parse(token)[0].normal_form for token in var] if lemmer else var\n",
    "        text = ' '.join(processed)\n",
    "        index, length = match(text.lower())\n",
    "        if length < best_score:\n",
    "            best_match = index\n",
    "            best_score = length\n",
    "    \n",
    "    print(f'{names[best_match]}, {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "10d773a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химера, 0.0\n",
      "CPU times: user 3.27 s, sys: 322 ms, total: 3.59 s\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('Химера')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "522a7b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химера, 0.0\n",
      "CPU times: user 38 s, sys: 5.08 s, total: 43.1 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('Сколько оттяжек нужно на Химере')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "38aaf01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химия любви, 0.0\n",
      "CPU times: user 38 s, sys: 4.88 s, total: 42.9 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('Какая категория у химии любви')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "80e1ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мисячне сяйво, 0.6415832042694092\n",
      "CPU times: user 38.2 s, sys: 5.03 s, total: 43.3 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('Сколько питчей у мисячне сяйва')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e948d4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Химия любви, 0.0\n",
      "CPU times: user 1min 52s, sys: 15.1 s, total: 2min 7s\n",
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_sentence('расскажи мне о том, Какая категория у химии любви')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc515c9",
   "metadata": {},
   "source": [
    "### Теперь комбинируем с SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "71682940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(sentence, pdist=torch.nn.PairwiseDistance(p=8.0)):\n",
    "    encoded_input = tokenizer(sentence, max_length=20, truncation=True, \n",
    "                   padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded_input)\n",
    "    \n",
    "    populated_result = torch.cat(titles_vectorized_tensor.shape[0]*[out[1]])\n",
    "    distances = pdist(populated_result, titles_vectorized_tensor).numpy()\n",
    "    length = np.min(distances)\n",
    "    index = np.argmin(distances)\n",
    "    return index, length\n",
    "\n",
    "def variants(sentence):\n",
    "    result = []\n",
    "    splitted = sentence.split()\n",
    "    for i in range(len(splitted)):\n",
    "        left_slice = splitted[i:]\n",
    "        for j in range(i+1, len(left_slice) + i + 1):\n",
    "            result.append(splitted[i:j])\n",
    "    return result\n",
    "    \n",
    "def match_sentence(sentence, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    perms = variants(sentence)\n",
    "    best_match, best_score = -1, 999999\n",
    "    for var in perms:\n",
    "        processed = [lemmer.parse(token)[0].normal_form for token in var] if lemmer else var\n",
    "        text = ' '.join(processed)\n",
    "        index, length = match(text.lower())\n",
    "        if length < best_score:\n",
    "            best_match = index\n",
    "            best_score = length\n",
    "    \n",
    "    return best_match, best_score\n",
    "\n",
    "def answer(question):\n",
    "    index, score = match_sentence(question)\n",
    "    answer = squad_model([text[index]], [question])\n",
    "    print(f'Маршрут: \"{names[index]}\"')\n",
    "    print(answer[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e219ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Химия любви\"\n",
      "Спорт  6c\n"
     ]
    }
   ],
   "source": [
    "answer('Химия Любви')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "87592544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Химия любви\"\n",
      "в Крым Симеиз\n"
     ]
    }
   ],
   "source": [
    "answer('Где находится Химия Любви')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fa8632f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Химия любви\"\n",
      "сколько нужно оттяжек, нет информации о том, сколько\n"
     ]
    }
   ],
   "source": [
    "answer('сколько метров веревки нужно на Химии Любви')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "09dbf0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Химера\"\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "answer('сколько оттяжек нужно на химере')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "eb5db2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Химера\"\n",
      "в Крым Никита D. Юг\n"
     ]
    }
   ],
   "source": [
    "answer('где находится маршрут химера')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "831958f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Кадавр\"\n",
      "в Крым Красный Камень C\n"
     ]
    }
   ],
   "source": [
    "answer('где находится кадавр')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8f39cb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Кадавр\"\n",
      "Спорт  7c 14м 9 болтов\n"
     ]
    }
   ],
   "source": [
    "answer('маршрут кадавр')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2f46dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Кадавр\"\n",
      "14 метров\n"
     ]
    }
   ],
   "source": [
    "answer('сколько веревки нужно на кадавре')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "09b50fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Кадавр\"\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "answer('сколько оттяжек нужно на кадавре')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4fe4a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Солнце над Типи\"\n",
      "Карабин\n"
     ]
    }
   ],
   "source": [
    "answer('какая станция на солнце над типи')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8ccb191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Автострада\"\n",
      "Спорт  6b 20м 13 болтов, станция Карабин\n"
     ]
    }
   ],
   "source": [
    "answer('автострада')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fa547cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Автострада\"\n",
      "в Крым Симеиз\n"
     ]
    }
   ],
   "source": [
    "answer('где находится автострада')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a8e4479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Автострада\"\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "answer('сколько пролазов на автостраде')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d4b34ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Автострада\"\n",
      "69 людям\n"
     ]
    }
   ],
   "source": [
    "answer('кому нравится автострада')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2f57264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Never kiss a dog\"\n",
      "Спорт  7a 17м 8 болтов\n"
     ]
    }
   ],
   "source": [
    "answer('never kiss a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ab6da0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Never kiss a dog\"\n",
      "в Antalya Geyikbayırı Gizmo 1\n"
     ]
    }
   ],
   "source": [
    "answer('где находится never kiss a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1bd2f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Сиртаки\"\n",
      "1 людей пролезло\n"
     ]
    }
   ],
   "source": [
    "answer('курцы умирают рано')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "17310298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Геликоптер\"\n",
      "Спорт  7c 21м 10 болтов, станция Карабин\n"
     ]
    }
   ],
   "source": [
    "answer('геликоптер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "228ba752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Геликоптер\"\n",
      "в Крым Никита B\n"
     ]
    }
   ],
   "source": [
    "answer('где находится геликоптер')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "825113db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Розовая\"\n",
      "в Москва\n"
     ]
    }
   ],
   "source": [
    "answer('где находится розовый фламинго')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e03fb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так, одно слово может перебить сочетания.. это нехорошо. будем ранжировать с учетом сочетаний слов\n",
    "# У подходящих сочетаниц приоритет выше чем у подходящих единичных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a843915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(sentence, pdist=torch.nn.PairwiseDistance(p=8.0)):\n",
    "    encoded_input = tokenizer(sentence, max_length=20, truncation=True, \n",
    "                   padding='max_length', return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded_input)\n",
    "    \n",
    "    populated_result = torch.cat(titles_vectorized_tensor.shape[0]*[out[1]])\n",
    "    distances = pdist(populated_result, titles_vectorized_tensor).numpy()\n",
    "    length = np.min(distances)\n",
    "    index = np.argmin(distances)\n",
    "    return index, length\n",
    "\n",
    "def variants(sentence):\n",
    "    result = []\n",
    "    splitted = sentence.split()\n",
    "    for i in range(len(splitted)):\n",
    "        left_slice = splitted[i:]\n",
    "        for j in range(i+1, len(left_slice) + i + 1):\n",
    "            result.append(splitted[i:j])\n",
    "    return result\n",
    "    \n",
    "def match_sentence(sentence, lemmer=pymorphy2.MorphAnalyzer()):\n",
    "    perms = variants(sentence)\n",
    "    best_match, best_score = -1, (999999, 0)\n",
    "    for var in perms:\n",
    "        processed = [lemmer.parse(token)[0].normal_form for token in var] if lemmer else var\n",
    "        text = ' '.join(processed)\n",
    "        index, length = match(text.lower())\n",
    "        \n",
    "        if length < best_score[0]:\n",
    "            best_match = index\n",
    "            best_score = (length, len(processed))\n",
    "        elif length == best_score[0]:\n",
    "            if len(processed) > best_score[1]:\n",
    "                best_match = index\n",
    "                best_score = (length, len(processed))\n",
    "    \n",
    "    return best_match, best_score\n",
    "\n",
    "def answer(question):\n",
    "    index, score = match_sentence(question)\n",
    "    answer = squad_model([text[index]], [question])\n",
    "    print(f'Маршрут: \"{names[index]}\"')\n",
    "    print(answer[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "feda4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Розовый фламинго\"\n",
      "в Крым Никита F\n"
     ]
    }
   ],
   "source": [
    "answer('где находится розовый фламинго')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b1d93fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Розовый фламинго\"\n",
      "12 метров\n"
     ]
    }
   ],
   "source": [
    "answer('сколько веревки нужно на розовом фламинго')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7bc110c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Крымский геккон\"\n",
      "12 метров\n",
      "CPU times: user 42 s, sys: 5.08 s, total: 47.1 s\n",
      "Wall time: 6.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "answer('сколько веревки нужно на розовом крымском гекконе')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "63361b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61f9ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Крымский геккон\"\n",
      "12 метров\n",
      "CPU times: user 28.8 s, sys: 3.43 s, total: 32.3 s\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "answer('сколько веревки нужно на розовом крымском гекконе')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "874f6d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Шепот в ночи\"\n",
      "в Крым Симеиз\n"
     ]
    }
   ],
   "source": [
    "answer('где находится шёпот в ночи')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bd9dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Шепот в ночи\"\n",
      "Спорт  6a+/7a 26м 15 болтов\n"
     ]
    }
   ],
   "source": [
    "answer('маршрут шёпот в ночи')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "01955b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маршрут: \"Голубые ели\"\n",
      "Цепь\n"
     ]
    }
   ],
   "source": [
    "answer('какая станция на маршруте голубые ели')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e0ca863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''], [-1], [0.4366404414176941]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_model([text[0]], ['крымский геккон'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8555f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81826e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f75b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f89da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fc672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cef50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
